## 선형 회귀(Linear Regression) 모음

<br/>

## 1. 작동 원리 : 단순선형회귀

<br/>

### ① 기초 개념

<br/>

- 선형회귀란 x와 y의 선형적인 관계를 구해 값을 예측하는 것이다.

- x는 어떤 값이은 적용될 수 있으며 기울기와 절편의 값을 알게 된다면, 원하는 x값을 대입했을 때 y값을 얻을 수 있다 (y = ax+b)      

- 인공지능에서는 기울기 a를 w(weight)라는 변수로 표현하며, 가중치 또는 계수라고 부르고, b(bias)는 수학에서와 동일하게 절편이라고 부른다.

<br/>

![Linear Regression](image/Linear%20Regression_0.png)

<br/>

### ② 가중치와 계수 구하기

<br/>

- 선형회귀에서는 가중치와 절편을  평균제곱오차 (Mean Squared Error - MSE)를 최소화하는 방법으로 구할 수 있다.

- 평균제곱오차란, 데이터의 오차를 제곱하여 데이터의 개수만큼 나눈 값을 의미한다.

<br/>

- 아래 그래프를 참고하면 그래프에서 오차가 5인 점(dot)을 살펴보면 실제 데이터의 값은 9이지만, 방정식은 4라고 인지하고 있다. 이럴 경우, 오차는 +5가 된다.

- 그 다음으로 오차가 -2인 점을 살펴보면, 실제 값은 3인데 방정식에서는 5라고 인지하고 있다.

- 이렇게 오차는 음수가 될 수도 있고, 양수가 될 수도 있기 때문에 오차에 제곱을 해주는 것이며,

- 오차의 평균치를 구하기 위해 데이터의 개수로 나누어 주는 것이다.

- 이 오차를 최소화하는 방정식을 구한다면, 우리는 y값을 예측할 수 있는 최적의 모델을 만들어낼 수 있다.

<br/>

![Linear Regression](image/Linear%20Regression_1.png)

<br/>

- 예를 들어, 많은 양의 생선 무게와 길이 데이터셋(Data Set)을 가지고 있는 상태에서, 특정 생선의 길이만으로 이 생선의 무게가 몇g 인지 알아보고 싶다고 가정한다.

- 일반적으로 생선의 길이가 길 수록 무게는 많이 나갈 것이다. 즉, 비례관계라는 것이다.

- 그렇다면 생선의 길이를 구하는 임의의 방정식은 아래와 같이 세울 수 있다.

  [생선의 길이 = weight  생선의무게 + b]

- 기존 생선의 무게와 길이는 알고있으니 인공지능에게 학습을 시켜 평균제곱오차를 최소화하는 최적의 w(가중치)와 b(절편)를 찾아내기만 하면 된다.

- 이 과정을 거친다면 자연스럽게 생선의 무게만으로 길이까지 예측할 수 있을 것이다.

<br/><br/>

## 2. 다항회귀

<br/>

### ① 기초개념

<br/>

- 데이터를 표현함에 있어 선형이 아닌 곡선으로 나타나는 경우에 사용하는 회귀이다.

- 단순히 비례관계로 표현된다면 너무나도 편하겠지만, 수많은 데이터를 표현할 때 그렇지 않은 경우가 더 많기 때문이다.

- 이러한 경우에 선택할 수 있는 회귀법이 바로 다항회귀이다.

- 다항회귀에서의 항은 차수(degree)를 뜻하는데 말 그대로 차수가 많다는 것을 의미한다.

- 차수(degree)를 높였을 때의 장점은 모델(model)이 기존의 모델보다 더 적은 오차를 만들어낸다는 점이다.

<br/>

![Linear Regression](image/Linear%20Regression_2.png)

<br/>

### ② 다항회귀 표현하기

<br/>

- 머신러닝에서는 결과값(target)을 구하기 위한 입력 데이터를 특징(feature)이라고 표현한다.

- 생선의 무게를 예측하기 위해 사용한 데이터는 생선의 길이였다.

- 생선의 길이는 우리가 입력 값으로 사용을 하였는데, 이는 다시 말해, 생선의 길이는 생선의 무게를 구하기 위한 하나의 특징(feature)이라고 말 할 수 있다.

- 결국 이 생선의 길이라는 특징을 변형 시켜 다항으로 표현해내면 되는 것이다.

- 위에서 다항의 항은 차수를 의미한다고 설명하였으니 위의 식처럼, 생선의 길이에 차수를 부여하여 여러 특징을 만들어내면 된다.

- 차수가 1인 생선의 길이라는 단순한 특징을 제곱하여 또 다른 특징을 만들어낼 수 있게 되는 것이다.

- 인공지능도 특징이 많으면 많을 수록 정답을 더 정확히 맞추게 된다고 생각한다.

  [생선의무게 = (w1  생선의길이) + (w2  생선의 길이²) +... (w100   생선의길이) + b]

<br/>

- 하지만, 너무 많은 차수를 사용한다면 오히려 과대적합(Overfitting)을 불러일으킬 수 있으니 주의하여야 한다.

- 이를 위해, 사이킷런(sklearn)에서는 과대적합을 막으면서 차수를 자동으로 적절히 조절할 수 있는 릿지(Ridge)와 라쏘(Lasso)라는 규제(regularization) 클래스를 제공한다.

- 여기에서 규제(regularization)란, 과대적합을 막을 수 있도록 모델의 파라미터(Parameter)를 조정하는 것을 뜻한다. 즉, '이 정도 차수면 충분해!' 라고 말해주는 것이다.

<br/><br/>

## 3. 릿지 / 라쏘

<br/>

- 모델이 과적합되게 학습하지 않고 일반성을 가질 수 있도록 규제 하는 것을 뜻한다.

- 아래 그래프의 하늘색 선은 오버피팅 될 수 있으므로 빨간색 점선으로 모델이 설정될 수 있게 해주는 작업이다.

- 데이터의 feature에는 손대지 않고 최대한 하늘색 선을 펴주려면 기울기(가중치)를 건드리면 된다.

- 이때 이용하는 것이 Lasso, Ridge이다.

<br/>

![Linear Regression](image/Linear%20Regression_3.png)

<br/>

### ① 릿지 (Ridge)

<br/>

- 모델의 손실 함수 최소화라는 목표에 추가로 가중치에 대한 목표를 만든다.

- 가중치의 절댓값을 최대한 작게 만드는 것을 목표로 한다. 이는 결국 기울기를 작게 만드는 것이다. 

- 각 가중치 제곱합에 규제 강도 α를 곱하여 오차에 더하는 것으로, `L2 규제`라고도 한다.

- 결론 : 릿지(Ridge)는 변형된 데이터 변수를 모두 사용하되 계수를 줄이는 방식을 사용하고, 많은 독립변수의 선형결합으로 모델을 생성하기 때문에 이 독립변수들의 설명력이 서로 차이가 크게 나지 않을수록 우수한 성능을 보여준다

<br/>

### ② 라쏘 (Lasso)

<br/>

- Ridge 규제와 같이 가중치를 최소화하는 것을 목표로 한다.

- 가중치의 제곱합이 아닌 가중치의 합에 α를 곱하여 오차에 더하는 것으로 `l1 규제`라고도 한다. 

- `L2 규제`와는 다르게 어떤 가중치 `w` 는 0이 될 수 있다. 즉 모델에서 완전히 제외되는 특성이 생길 수 있다.

- 결론 : 라쏘(Lasso)는 일정 변수들의 계수를 0으로 만들어 사용하지 않도록 하는 방법을 택한다. 따라서, 데이터가 손실되어 정확도가 떨어지는 위험성이 있기도 하다.
    
    ⇒ 라쏘 회귀분석에서는 일부 독립변수를 제거할 수 있기 떄문에, 일부 독립변수의 설명력이 크고, 나머지 독립변수의 설명변수가 설명력이 낮을 때 우수한 성능을 보여준다.

<br/>

### ③ 엘라스틱넷 (ElasticNet)

<br/>

- Lasso와 Ridge를 결합한 것으로, L1과 L2 규제의 비율을 설정해주어야 한다.

- 릿지 회귀모델처럼 모델을 충분하게 설명하지 못하는 일부 독립변수의 회귀계수 크기를 축소할 수 있고,

- 랏쏘 회귀모델처럼 모델을 충분하게 설명하기 못하는 일부 독립변수의 회귀계수를 강제로 0으로 할당하여 특정 독립변수를 모델에서 제거할 수 있다.
